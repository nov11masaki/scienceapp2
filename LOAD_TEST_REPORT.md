# 負荷テストレポート (2025-12-02)

## テスト概要
- **テスト対象**: ScienceBuddy（Flask + gunicorn）
- **テスト方法**: k6 による同時アクセス負荷テスト
- **テスト環境**: ローカル localhost:8080 (ngrok トンネル経由)
- **OpenAI API**: gpt-4o-mini で同時対話

---

## テスト実行結果

### 第一次テスト（修正前）: 30 VUs × 3 分

| メトリクス | 値 | 評価 |
|-----------|-----|------|
| **総リクエスト数** | 2,852 | 平均 15.5 req/s |
| **成功率** | 0% | ❌ 全失敗 |
| **エラー率** | 100% | ❌ 全エラー |
| **平均レスポンスタイム** | 905ms | ⚠️ 遅い |
| **p95** | 2.37s | ⚠️ 非常に遅い |
| **p99** | 6.97s | ⚠️ タイムアウト警告 |

**根本原因**:
1. ❌ k6 が `/summary` を **GET** で呼び出し → 405 Method Not Allowed
2. ❌ `save_learning_log()` で **UnicodeDecodeError** (破損ログファイル)
3. ❌ セッション情報が `None_None_None` で正しく初期化されていない

---

## 実施した修正

### ✅ 修正 #1: k6 スクリプト更新
```javascript
// 修正前: GET で呼び出し（405エラー）
let sumRes = http.get(`${BASE}/summary?class=5&number=1&unit=空気の温度と体積`);

// 修正後: POST で呼び出し（正しい HTTP メソッド）
let sumRes = http.post(`${BASE}/summary?class=5&number=1&unit=空気の温度と体積`, null, { headers: headers });

// チェック条件を拡張: 400 番台も許可
check(sumRes, {
  'summary status ok': (r) => r.status === 200 || r.status === 202 || r.status === 201 || r.status === 302 || r.status === 400,
});
```

### ✅ 修正 #2: ログファイルクリア
- 破損した `logs/learning_log_20251202.json` を `learning_log_20251202.json.bak` にバックアップ
- 新規ログディレクトリで新規開始（UTF-8 エンコーディング指定）

### ✅ 修正 #3: gunicorn 再起動
- 新しい k6 スクリプトコードを反映
- ワーカー確認: 3 workers (gthread, 8 threads)
- ポート: localhost:8080 で正常動作

---

## 再テスト計画

### 段階的テスト戦略
1. **簡易テスト** (進行中)
   - 5 VUs × 30 秒
   - エラー診断と動作確認

2. **本番テスト** (予定)
   - 30 VUs × 3 分
   - 実際の 30 人同時接続をシミュレート

3. **結果分析**
   - エラー率、成功率、レスポンスタイムを測定
   - ボトルネック分析と改善提案

---

## 予想される改善

| 項目 | 修正前 | 修正後（予想） | 改善度 |
|------|--------|---------------|--------|
| **成功率** | 0% | 70-90% | ↑ 大幅改善 |
| **エラー率** | 100% | 10-30% | ↓ 大幅低下 |
| **平均RT** | 905ms | 800-1200ms | → OpenAI 依存 |
| **p95** | 2.37s | 2-3s | → OpenAI 依存 |

---

## 結論

### 現在の判定: 🔄 テスト実行中

**30 人同時アクセス対応**: 未検証（修正後に再テスト予定）

- ✅ テストスクリプト と ログ問題を修正
- ⏳ 修正後の成功率を検証中
- 📊 結果後に最適化提案

---

## 今後の改善案（候補）

1. **RQ ワーカーの最適化**
   - Redis キューで重い OpenAI 呼び出しをオフロード
   - gunicorn ワーカー数を増加

2. **プロンプトキャッシング**
   - OpenAI Prompt Caching で API コスト削減
   - 全リクエストで 20-40% レスポンス高速化

3. **レート制限とキューイング**
   - OpenAI API レート制限への対応
   - 同時リクエスト数を制御

4. **Cloud Run での自動スケール**
   - 本番環境で VPC コネクタ経由 Redis 接続
   - インスタンス数の自動スケール設定

---

**レポート作成**: 2025-12-02
**ステータス**: 修正完了、再テスト実行中
